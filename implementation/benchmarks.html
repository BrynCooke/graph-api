<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Benchmarking - graph-api</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../theme/custom.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">graph-api</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="benchmarking"><a class="header" href="#benchmarking">Benchmarking</a></h1>
<p>Benchmarking is essential for measuring and comparing the performance of your graph implementation. This chapter covers how to set up and run benchmarks, interpret results, and use benchmarking to guide optimization.</p>
<h2 id="setting-up-benchmarks"><a class="header" href="#setting-up-benchmarks">Setting Up Benchmarks</a></h2>
<p>The Graph API provides built-in benchmarking tools to help you evaluate your implementation's performance.</p>
<h3 id="adding-benchmarking-dependencies"><a class="header" href="#adding-benchmarking-dependencies">Adding Benchmarking Dependencies</a></h3>
<p>First, add the necessary dependencies to your <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[dev-dependencies]
criterion = "0.5"
graph-api-benches = { path = "../graph-api-benches" }

[[bench]]
name = "my_graph_benchmarks"
harness = false
</code></pre>
<h3 id="creating-a-basic-benchmark-suite"><a class="header" href="#creating-a-basic-benchmark-suite">Creating a Basic Benchmark Suite</a></h3>
<p>Create a benchmark file in your project's <code>benches</code> directory:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// benches/my_graph_benchmarks.rs
use criterion::{criterion_group, criterion_main, Criterion};
use graph_api_benches::bench_suite;
use my_graph::MyGraph;

fn criterion_benchmark(c: &amp;mut Criterion) {
    bench_suite!(c, || MyGraph::new());
}

criterion_group!(benches, criterion_benchmark);
criterion_main!(benches);
<span class="boring">}</span></code></pre></pre>
<p>The <code>bench_suite!</code> macro runs a standardized set of benchmarks against your graph implementation.</p>
<h2 id="standard-benchmark-suite"><a class="header" href="#standard-benchmark-suite">Standard Benchmark Suite</a></h2>
<p>The standard benchmark suite in <code>graph-api-benches</code> tests several aspects of graph performance:</p>
<h3 id="1-construction-benchmarks"><a class="header" href="#1-construction-benchmarks">1. Construction Benchmarks</a></h3>
<p>Measures the performance of creating graph elements:</p>
<ul>
<li>Creating vertices</li>
<li>Creating edges</li>
<li>Creating graphs of different sizes</li>
</ul>
<h3 id="2-query-benchmarks"><a class="header" href="#2-query-benchmarks">2. Query Benchmarks</a></h3>
<p>Evaluates lookup and traversal performance:</p>
<ul>
<li>Vertex retrieval by ID</li>
<li>Vertex retrieval by index</li>
<li>Edge traversal</li>
<li>Path finding</li>
</ul>
<h3 id="3-mutation-benchmarks"><a class="header" href="#3-mutation-benchmarks">3. Mutation Benchmarks</a></h3>
<p>Tests the efficiency of modifying the graph:</p>
<ul>
<li>Adding vertices and edges</li>
<li>Removing vertices and edges</li>
<li>Modifying vertex and edge properties</li>
</ul>
<h3 id="4-traversal-benchmarks"><a class="header" href="#4-traversal-benchmarks">4. Traversal Benchmarks</a></h3>
<p>Measures the performance of walking the graph:</p>
<ul>
<li>Simple steps (vertices, edges)</li>
<li>Filter operations</li>
<li>Map operations</li>
<li>Complex traversals</li>
</ul>
<h3 id="5-scale-benchmarks"><a class="header" href="#5-scale-benchmarks">5. Scale Benchmarks</a></h3>
<p>Assesses how performance scales with graph size:</p>
<ul>
<li>Small graphs (10s of vertices)</li>
<li>Medium graphs (100s of vertices)</li>
<li>Large graphs (1000s of vertices)</li>
<li>Huge graphs (10,000s of vertices)</li>
</ul>
<h2 id="running-benchmarks"><a class="header" href="#running-benchmarks">Running Benchmarks</a></h2>
<p>To run the benchmarks:</p>
<pre><code class="language-bash"># Run all benchmarks
cargo bench

# Run a specific benchmark
cargo bench -- vertex_insertion

# Run benchmarks with more iterations for higher precision
CRITERION_SAMPLE_SIZE=100 cargo bench
</code></pre>
<h2 id="custom-benchmarks"><a class="header" href="#custom-benchmarks">Custom Benchmarks</a></h2>
<p>While the standard suite covers many scenarios, you should also create custom benchmarks for your implementation's specific features.</p>
<h3 id="example-custom-benchmark"><a class="header" href="#example-custom-benchmark">Example: Custom Benchmark</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn custom_benchmarks(c: &amp;mut Criterion) {
    // Benchmark for a specialized lookup method
    c.bench_function("custom_lookup", |b| {
        let mut graph = MyGraph::new();
        
        // Setup: Create a graph with test data
        let vertices = setup_test_graph(&amp;mut graph);
        
        b.iter(|| {
            // Benchmark your custom operation
            for vertex_id in &amp;vertices {
                graph.my_custom_lookup(*vertex_id);
            }
        })
    });
    
    // Benchmark for batch operations
    c.bench_function("batch_add_vertices", |b| {
        b.iter(|| {
            let mut graph = MyGraph::new();
            
            // Create test data
            let vertices = create_test_vertices(1000);
            
            // Benchmark the batch add operation
            graph.add_vertices_batch(vertices);
        })
    });
}

criterion_group!(custom_benches, custom_benchmarks);
<span class="boring">}</span></code></pre></pre>
<h2 id="comparative-benchmarking"><a class="header" href="#comparative-benchmarking">Comparative Benchmarking</a></h2>
<p>To compare your implementation against other graph implementations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn comparison_benchmarks(c: &amp;mut Criterion) {
    let mut group = c.benchmark_group("vertex_insertion");
    
    // Benchmark your implementation
    group.bench_function("my_graph", |b| {
        b.iter(|| {
            let mut graph = MyGraph::new();
            for i in 0..1000 {
                graph.add_vertex(create_test_vertex(i));
            }
        })
    });
    
    // Benchmark SimpleGraph
    group.bench_function("simple_graph", |b| {
        b.iter(|| {
            let mut graph = SimpleGraph::new();
            for i in 0..1000 {
                graph.add_vertex(create_test_vertex(i));
            }
        })
    });
    
    // Benchmark PetGraph
    group.bench_function("pet_graph", |b| {
        b.iter(|| {
            let mut graph = PetGraph::new();
            for i in 0..1000 {
                graph.add_vertex(create_test_vertex(i));
            }
        })
    });
    
    group.finish();
}

criterion_group!(comparison_benches, comparison_benchmarks);
<span class="boring">}</span></code></pre></pre>
<h2 id="interpreting-benchmark-results"><a class="header" href="#interpreting-benchmark-results">Interpreting Benchmark Results</a></h2>
<p>Criterion provides detailed statistics for each benchmark:</p>
<ul>
<li><strong>Mean time</strong>: Average execution time</li>
<li><strong>Median time</strong>: Middle value of all measurements</li>
<li><strong>Standard deviation</strong>: Variation in measurements</li>
<li><strong>Throughput</strong>: Operations per second (for parameterized benchmarks)</li>
</ul>
<h3 id="sample-output-analysis"><a class="header" href="#sample-output-analysis">Sample Output Analysis</a></h3>
<pre><code>Benchmarking vertex_insertion: Collecting 100 samples in estimated 5.2684 s (10k iterations)
vertex_insertion           time:   [524.44 µs 525.30 µs 526.33 µs]
                           thrpt:  [1.9000 Melem/s 1.9037 Melem/s 1.9068 Melem/s]
</code></pre>
<p>This output shows:</p>
<ul>
<li>The benchmark took around 525 microseconds per iteration</li>
<li>It processes about 1.9 million elements per second</li>
</ul>
<h3 id="comparing-results"><a class="header" href="#comparing-results">Comparing Results</a></h3>
<p>When comparing benchmark results:</p>
<ol>
<li><strong>Focus on relative differences</strong>: A 5-10% difference might be noise; look for larger gaps</li>
<li><strong>Consider operation complexity</strong>: Some operations naturally scale differently</li>
<li><strong>Check variance</strong>: High standard deviation suggests inconsistent performance</li>
<li><strong>Look at throughput</strong>: For scale benchmarks, throughput may be more meaningful than raw time</li>
</ol>
<h2 id="advanced-benchmarking-techniques"><a class="header" href="#advanced-benchmarking-techniques">Advanced Benchmarking Techniques</a></h2>
<h3 id="parameterized-benchmarks"><a class="header" href="#parameterized-benchmarks">Parameterized Benchmarks</a></h3>
<p>Test how performance scales with input size:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn parameterized_bench(c: &amp;mut Criterion) {
    let mut group = c.benchmark_group("vertex_count_scaling");
    
    for size in [10, 100, 1000, 10000].iter() {
        group.throughput(criterion::Throughput::Elements(*size as u64));
        group.bench_with_input(format!("vertices_{}", size), size, |b, &amp;size| {
            b.iter(|| {
                let mut graph = MyGraph::new();
                for i in 0..size {
                    graph.add_vertex(create_test_vertex(i));
                }
            });
        });
    }
    
    group.finish();
}
<span class="boring">}</span></code></pre></pre>
<h3 id="profiling-during-benchmarks"><a class="header" href="#profiling-during-benchmarks">Profiling During Benchmarks</a></h3>
<p>Use profiling tools with benchmarks:</p>
<pre><code class="language-bash"># Generate a flame graph during benchmark execution
cargo flamegraph --bench my_graph_benchmarks -- --bench vertex_insertion

# Use perf for detailed profiling
perf record -g cargo bench -- vertex_insertion
perf report
</code></pre>
<h2 id="real-world-benchmark-examples"><a class="header" href="#real-world-benchmark-examples">Real-World Benchmark Examples</a></h2>
<h3 id="memory-usage-benchmarking"><a class="header" href="#memory-usage-benchmarking">Memory Usage Benchmarking</a></h3>
<p>Measure memory consumption:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn memory_benchmark(c: &amp;mut Criterion) {
    c.bench_function("memory_usage_1M_vertices", |b| {
        b.iter_custom(|iters| {
            let mut total_duration = std::time::Duration::new(0, 0);
            
            for _ in 0..iters {
                // Record baseline memory
                let baseline = get_current_memory_usage();
                
                // Start timing
                let start = std::time::Instant::now();
                
                // Create a large graph
                let graph = create_large_graph(1_000_000);
                
                // Force graph to stay alive until measurement
                std::hint::black_box(&amp;graph);
                
                // Measure memory increase
                let peak = get_current_memory_usage();
                let memory_used = peak - baseline;
                
                // Log memory usage (not part of timing)
                println!("Memory used: {} MB", memory_used / (1024 * 1024));
                
                // End timing
                total_duration += start.elapsed();
            }
            
            total_duration
        });
    });
}

// Helper function to get memory usage
fn get_current_memory_usage() -&gt; usize {
    // Use platform-specific APIs to get memory usage
    // For example, using the `psutil` crate
    // ...
}
<span class="boring">}</span></code></pre></pre>
<h3 id="concurrent-access-benchmarking"><a class="header" href="#concurrent-access-benchmarking">Concurrent Access Benchmarking</a></h3>
<p>For graph implementations supporting concurrency:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn concurrent_benchmark(c: &amp;mut Criterion) {
    let mut group = c.benchmark_group("concurrent_access");
    
    for threads in [1, 2, 4, 8, 16].iter() {
        group.bench_function(format!("read_threads_{}", threads), |b| {
            // Create a shared graph
            let graph = Arc::new(MyGraph::new());
            populate_test_graph(&amp;graph);
            
            b.iter(|| {
                let mut handles = Vec::new();
                
                // Spawn reader threads
                for _ in 0..*threads {
                    let graph_clone = Arc::clone(&amp;graph);
                    handles.push(std::thread::spawn(move || {
                        // Perform concurrent read operations
                        for _ in 0..1000 {
                            let _ = graph_clone.walk()
                                .vertices(VertexSearch::scan())
                                .collect::&lt;Vec&lt;_&gt;&gt;();
                        }
                    }));
                }
                
                // Wait for all threads
                for handle in handles {
                    handle.join().unwrap();
                }
            });
        });
    }
    
    group.finish();
}
<span class="boring">}</span></code></pre></pre>
<h2 id="using-benchmarks-to-guide-optimization"><a class="header" href="#using-benchmarks-to-guide-optimization">Using Benchmarks to Guide Optimization</a></h2>
<p>Benchmarks are most valuable when used as part of your optimization process:</p>
<ol>
<li><strong>Establish a baseline</strong>: Run benchmarks before making any changes</li>
<li><strong>Identify bottlenecks</strong>: Use results to find which operations are slowest</li>
<li><strong>Make targeted improvements</strong>: Change one thing at a time</li>
<li><strong>Measure impact</strong>: Run benchmarks after each change</li>
<li><strong>Validate trade-offs</strong>: Ensure optimizations don't degrade other aspects</li>
</ol>
<h3 id="example-optimization-workflow"><a class="header" href="#example-optimization-workflow">Example Optimization Workflow</a></h3>
<pre><code>1. Initial benchmark:
   vertex_insertion: 525.30 µs

2. Profiling shows excessive allocations in vertex insertion
   - Modify implementation to reduce allocations

3. After optimization:
   vertex_insertion: 320.15 µs (39% improvement)

4. Check if other operations are affected:
   vertex_removal: 212.40 µs (unchanged)
   edge_insertion: 187.30 µs (5% improvement)
</code></pre>
<h2 id="benchmark-driven-development"><a class="header" href="#benchmark-driven-development">Benchmark-Driven Development</a></h2>
<p>Consider adopting benchmark-driven development for performance-critical components:</p>
<ol>
<li><strong>Write benchmarks first</strong>: Define performance expectations</li>
<li><strong>Implement the feature</strong>: Focus on correctness first</li>
<li><strong>Run benchmarks</strong>: Measure initial performance</li>
<li><strong>Optimize iteratively</strong>: Improve performance until targets are met</li>
<li><strong>Document results</strong>: Record performance characteristics</li>
</ol>
<h2 id="common-pitfalls-in-benchmarking"><a class="header" href="#common-pitfalls-in-benchmarking">Common Pitfalls in Benchmarking</a></h2>
<p>Watch out for these benchmarking issues:</p>
<ol>
<li><strong>Measurement noise</strong>: External factors affecting results</li>
<li><strong>Microbenchmark traps</strong>: Optimizing operations that don't matter in real use</li>
<li><strong>Compiler optimizations</strong>: Dead code elimination skewing results</li>
<li><strong>Benchmark bias</strong>: Testing only favorable scenarios</li>
<li><strong>Invalid comparisons</strong>: Comparing different operations as if they were equivalent</li>
</ol>
<h3 id="avoiding-benchmarking-mistakes"><a class="header" href="#avoiding-benchmarking-mistakes">Avoiding Benchmarking Mistakes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Allows dead code elimination
c.bench_function("ineffective_bench", |b| {
    b.iter(|| {
        let mut graph = MyGraph::new();
        for i in 0..1000 {
            graph.add_vertex(create_test_vertex(i));
        }
        // Results never used, may be optimized away
    })
});

// GOOD: Prevents optimization with black_box
c.bench_function("effective_bench", |b| {
    b.iter(|| {
        let mut graph = MyGraph::new();
        for i in 0..1000 {
            graph.add_vertex(create_test_vertex(i));
        }
        // Force evaluation
        criterion::black_box(&amp;graph);
    })
});
<span class="boring">}</span></code></pre></pre>
<h2 id="benchmarking-checklist"><a class="header" href="#benchmarking-checklist">Benchmarking Checklist</a></h2>
<p>Use this checklist when creating benchmarks:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Include representative operations</li>
<li><input disabled="" type="checkbox"/>
Test a range of input sizes</li>
<li><input disabled="" type="checkbox"/>
Include setup code outside timed sections</li>
<li><input disabled="" type="checkbox"/>
Use <code>black_box</code> to prevent dead code elimination</li>
<li><input disabled="" type="checkbox"/>
Run multiple iterations for statistical significance</li>
<li><input disabled="" type="checkbox"/>
Test on the target hardware when possible</li>
<li><input disabled="" type="checkbox"/>
Include comparison benchmarks against similar implementations</li>
<li><input disabled="" type="checkbox"/>
Document benchmark configurations and results</li>
</ul>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>Effective benchmarking is crucial for developing high-performance graph implementations. By creating comprehensive benchmarks, accurately measuring performance, and using results to guide optimization, you can ensure your graph implementation meets its performance requirements.</p>
<p>Remember that benchmarks are most valuable when they reflect real-world usage patterns. Focus on benchmarking operations that will be common in actual applications, and be cautious about optimizing solely for benchmark performance at the expense of code clarity or maintainability.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../implementation/performance.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../reference/api.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../implementation/performance.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../reference/api.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
